---
title: "Chapter 10 and 11"
author: "Marjorie Blanco"
date: "February 6, 2018"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(class)
library(FNN)
library(kknn)
library(ggplot2)
library(rpart)
library(C50)
library(rpart.plot)
```

```{r, include=FALSE}
plotHistFunc <- function(x, f = "dodgerblue", na.rm = TRUE, ...) {
  nm <- names(x)
  for (i in seq_along(nm)) {
    p <- ggplot(x,aes_string(x = nm[i])) + 
      geom_histogram(alpha = .5, fill = f) 
    print(p)
  }
}
```

# Chapter K-Nearest Negighbor

## Problem 12

12. Using the data in table 10.5, find the k-nearest neighbor for Record #10, using k = 3.

Answer: The k-nearest (using k=3) neighbor for record #10 is record #6. The predicted risk is `good loss` with a probability of 67%.

```{r}
#Table 10.5 Data

age <-c(22,33,28,51,25,39,54,55,50,66)
marital <-c("Single","Married","Other","Other","Single","Single","Single","Married","Married","Married")
income <-c(46156.98,24188.10,28787.34,23886.72,47281.44,33994.90,28716.50,49186.75,46726.50,36120.35)
risk <-c("Bad loss","Bad loss","Bad loss","Bad loss","Bad loss","Good risk","Good risk","Good risk","Good risk","Good risk") 

risk <-data.frame(age,marital,income,risk)

risk$married <- risk$marital == "Married"
risk$single <- risk$marital == "Single"

risk.train <- risk[1:9,c(1,3,5,6)]
risk.new <- risk[10, c(1,3,5:6)]
risk.class <- risk[1:9, 4]

#K-nearest Neighbor
knn <- knn(risk.train,
          risk.new,
          cl=risk.class,
          k=3,
          prob=TRUE)

knn
attr(knn,"nn.index")[1]

#Find the k-nearest neighbor for Record #10, using k = 3
risk[attr(knn,"nn.index")[1],]
```

## Problem 13

Using the ClassifyRiskdata set with predictors age, marital status, and income, and target variable risk, find the k-nearest neighbor for Record #1, using k = 2 and Euclidean distance.

Answer: The k-nearest (using k=1) neighbor for record #1 is record #2.  The predicted risk is `bad loss` with a probability of 100%.

```{r}
#Read ClassifyRisk 

classifyrisk <- read.csv("ClassifyRisk",
                        header=T)


classifyrisk$married <- classifyrisk$marital == "Married"
classifyrisk$single <- classifyrisk$marital == "Single"

risk.class <- classifyrisk[2:246,6]
risk.train <- classifyrisk[2:246, c(3,5,7,8)]
risk.new <- classifyrisk[1, c(3,5,7,8)]

risk.knn <- knn(risk.train,
                risk.new,
                cl=risk.class,
                prob=TRUE,
                k = 2)

risk.knn
attr(risk.knn,"nn.index")[1]


#Find the record that is the nearest neighbor to Record #1 according to Euclidean Distance
risk.train[attr(risk.knn,"nn.index")[1],]
```

## Problem 14

Using the ClassifyRiskdata set with predictors age, marital status, and income, and target variable risk, find the k-nearest neighbor for Record #1, using k = 2 and Minkowski (city-block) distance (Chapter 19).

Answer: The k-nearest (using k=1) neighbor for record #1 is record #90.  The predicted risk is `bad loss` with a probability of 100%.

```{r}
risk.train <-classifyrisk[2:246, c(3,5:8)]

risk.new <-classifyrisk[1, c(3,5,7,8)]

risk.knn <- kknn(risk~.,
                 risk.train,
                 risk.new,
                 k = 2,
                 distance = 1)

risk.knn
summary(risk.knn)
risk.knn$prob
risk.knn$D
risk.knn$C[1]

#Find the record that is the nearest neighbor to Record #1 (using k=2) according to Minkowski Distance
risk.train[risk.knn$C[1],]
```

# Decission Trees

## Problem 11

Generate a CART decision tree.

```{r}
 
#Read churn table

churn <-read.table("churn.txt", 
                   header = T, 
                   sep=',', na.strings=c('','NA'))
names(churn) <- c("State","Account.Length","Area.Code","Phone","Intl.Plan","VMail.Plan","VMail.Message","Day.Mins" ,"Day.Calls"  ,"Day.Charge","Eve.Mins" ,"Eve.Calls","Eve.Charge","Night.Mins","Night.Calls" ,"Night.Charge","Intl.Mins" ,"Intl.Calls","Intl.Charge","CustServ.Calls" ,"Churn")
```

Normalize numerical data

```{r}
numeric.index <- unlist(lapply(churn, is.numeric)) 
#plotHistFunc(churn[,numeric.index])

#Normalize
churn.z <- data.frame(lapply(churn[,numeric.index], function(x) scale(x, center = FALSE, scale = TRUE)))

churn.z$Area.Code <- NULL
colnames(churn.z) <- paste("z", colnames(churn.z), sep = "_")

#plotHistFunc(churn.z)
```


```{r}
#Create CART decision tree
churn.z$Churn <- churn$Churn

churn.CART <-rpart(Churn ~ ., 
                   data = churn.z, 
                   method = "class")
print(churn.CART)

summary(churn.CART)

churnfit <- rpart(Churn ~ ., 
                  data = churn.z,
                  method="class")
rpart.plot(churnfit)
```

## Problem 12

Generate a C4.5-type decision tree.

Answer: This C4.5 decission tree contains 31 <TBD>

```{r}
churn <- cbind(churn, churn.z)
x <- churn[,c(5:6,22:36)]
y <- churn$Churn

#Create C4.5 decision tree

churn.C50 <- C5.0(x,y)
print(churn.C50)
```

## Problem 13

CART decision tree has a total of 83 nodes

Root node splits occur on Day.Mins

Day.Mins < 1.408195 on left and Day.Mins >= 1.408195 on the right

Creates split where 3119/208 have Day.Mins < 1.408195

CART considers Day.Mins most important attribute in classifying churn

Next most important is CustServ.Calls (Day.Mins < 1.408195) and Mail.Message (Day.Mins >= 1.408195)

C4.5 decision tree has a total of 31 nodes

Root node splits occur on Day.Mins

Day.Mins > 1.407928 on left and Day.Mins <= 1.407928 on the right

Next most important is CustServ.Calls (Day.Mins <= 1.407928) and VMail.Plan (Day.Mins > 1.407928)

The Attribute usage:
- 100.00% Day.Mins
- 93.72% CustServ.Calls
- 90.63% Intl.Plan

Has a 3% error

|(a)|(b)|<-classified as|
|----|----|----|
|2825|20|(a): class False|
|100|383|(b): class True|


Two algorithms do not agree in details

Do not agree on relative importance of their ordering in trees

Both agree that Day.Mins and CustServ.Calls are important attributes 

## Problem 14

Generate the full set of decision rules for the CART decision tree

```{r}
summary(churn.CART)
```

## Problem 15

Generate the full set of decision rules for the C4.5 decision tree.

```{r}
summary(churn.C50)
```

## Problem 16

Two algorithms do not agree in details

Do not agree on relative importance of their ordering in trees

CART Variable importance
- Day.Mins
- Day.Charge
- CustServ.Calls
- Eve.Mins 


C4.5 Variable importance
- 100.00% Day.Mins
- 93.72% CustServ.Calls
- 90.63% Intl.Plan
